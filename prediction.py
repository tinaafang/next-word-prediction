"""Display the ten most probable next token for GPT2 and GPT2 Large given inputs "Ludwig the cat"."""

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
import warnings
import logging

warnings.filterwarnings('ignore')
logging.getLogger().setLevel(logging.CRITICAL)


device = 'cpu'
if torch.cuda.is_available():
    device = 'cuda'
elif torch.backends.mps.is_available():
    device = 'mps'


def prediction(gpt2: bool):
    """Print out the ten most probable next token given inputs "Ludwig the cat"."""
    # set tokenizer and model
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
    if not gpt2:
        tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")
        model = GPT2LMHeadModel.from_pretrained("gpt2-large").to(device)

    # run the model
    input_ids = tokenizer(["Ludwig the cat"], return_tensors="pt").input_ids.to(device)
    outputs = model(input_ids)
    # get unnormalized probabilities
    next_token_logits = outputs.logits[0, -1, :]
    # normalize
    next_token_probs = torch.softmax(next_token_logits, -1)
    # get top 10 probabilities
    ten_next_tokens = torch.topk(next_token_probs, 10)

    # print out results
    model_name = 'GPT2' if gpt2 else 'GPT2 Large'
    print(f"\n10 most probable next tokens generated by {model_name}:")
    print(f"|{'Token':7s}|{'Probablity':10s} |")
    for idx, prob in zip(ten_next_tokens.indices, ten_next_tokens.values):
        print(f"|{tokenizer.decode(idx):7s}|{prob:<10.2%} |")


if __name__ == '__main__':
    prediction(gpt2=True)
    print("\n---------------------------------------------------")
    prediction(gpt2=False)
